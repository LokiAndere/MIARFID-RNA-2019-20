{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MNIST",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokiAndere/MIARFID-RNA-2019-20/blob/master/Copy_of_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzwiVN0hK95i",
        "colab_type": "code",
        "outputId": "c26403d4-4fae-49a4-c9b2-034aadda170d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers import Dropout, SpatialDropout2D, Flatten\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from math import exp\n",
        "\n",
        "batch_size = 75\n",
        "num_classes = 10\n",
        "epochs = 150\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "# Mandatory to use ImageDataGenerator, it expects 4D Tensors\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "## Data Augmentation with an ImageGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=35,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "## Model, note the reshape\n",
        "model = Sequential()\n",
        "model.add(Reshape(target_shape=(784,), input_shape=(28,28,1)))\n",
        "model.add(GN(0.3))\n",
        "#model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "#model.add(BN())\n",
        "#model.add(Activation('relu'))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "#model.add(BN())\n",
        "#model.add(Activation('relu'))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "#model.add(BN())\n",
        "#model.add(Activation('relu'))\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd=SGD(lr=0.1, decay=0.0, momentum=0.0)\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 50:\n",
        "        return 0.1\n",
        "    elif epoch < 100:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "set_lr = LRS(scheduler)\n",
        "\n",
        "mcp_save1 = ModelCheckpoint ('drive/My Drive/ggg.hdf5', save_best_only = True, monitor = 'val_acc', mode = 'auto')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[set_lr, mcp_save1],\n",
        "                            verbose=1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_2 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_6 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_7 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_8 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n",
            "Epoch 1/150\n",
            "800/800 [==============================] - 21s 26ms/step - loss: 1.6491 - acc: 0.4369 - val_loss: 0.4704 - val_acc: 0.8655\n",
            "Epoch 2/150\n",
            "800/800 [==============================] - 19s 23ms/step - loss: 1.1351 - acc: 0.6231 - val_loss: 0.3380 - val_acc: 0.8926\n",
            "Epoch 3/150\n",
            "800/800 [==============================] - 19s 23ms/step - loss: 0.9534 - acc: 0.6863 - val_loss: 0.2585 - val_acc: 0.9159\n",
            "Epoch 4/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.8534 - acc: 0.7190 - val_loss: 0.2200 - val_acc: 0.9310\n",
            "Epoch 5/150\n",
            "800/800 [==============================] - 19s 23ms/step - loss: 0.7965 - acc: 0.7380 - val_loss: 0.2234 - val_acc: 0.9271\n",
            "Epoch 6/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.7379 - acc: 0.7578 - val_loss: 0.1829 - val_acc: 0.9417\n",
            "Epoch 7/150\n",
            "800/800 [==============================] - 19s 23ms/step - loss: 0.6971 - acc: 0.7730 - val_loss: 0.1716 - val_acc: 0.9440\n",
            "Epoch 8/150\n",
            "800/800 [==============================] - 19s 23ms/step - loss: 0.6570 - acc: 0.7835 - val_loss: 0.1884 - val_acc: 0.9359\n",
            "Epoch 9/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.6386 - acc: 0.7905 - val_loss: 0.1642 - val_acc: 0.9481\n",
            "Epoch 10/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.6137 - acc: 0.7977 - val_loss: 0.1655 - val_acc: 0.9461\n",
            "Epoch 11/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5884 - acc: 0.8067 - val_loss: 0.1491 - val_acc: 0.9514\n",
            "Epoch 12/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5772 - acc: 0.8109 - val_loss: 0.1508 - val_acc: 0.9511\n",
            "Epoch 13/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5571 - acc: 0.8182 - val_loss: 0.1418 - val_acc: 0.9515\n",
            "Epoch 14/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5463 - acc: 0.8217 - val_loss: 0.1461 - val_acc: 0.9513\n",
            "Epoch 15/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5302 - acc: 0.8263 - val_loss: 0.1369 - val_acc: 0.9544\n",
            "Epoch 16/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5186 - acc: 0.8309 - val_loss: 0.1361 - val_acc: 0.9529\n",
            "Epoch 17/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5039 - acc: 0.8354 - val_loss: 0.1261 - val_acc: 0.9576\n",
            "Epoch 18/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.5002 - acc: 0.8369 - val_loss: 0.1043 - val_acc: 0.9642\n",
            "Epoch 19/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4912 - acc: 0.8395 - val_loss: 0.1213 - val_acc: 0.9600\n",
            "Epoch 20/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4810 - acc: 0.8427 - val_loss: 0.1142 - val_acc: 0.9611\n",
            "Epoch 21/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4759 - acc: 0.8475 - val_loss: 0.1180 - val_acc: 0.9599\n",
            "Epoch 22/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4657 - acc: 0.8472 - val_loss: 0.1134 - val_acc: 0.9637\n",
            "Epoch 23/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4586 - acc: 0.8499 - val_loss: 0.1115 - val_acc: 0.9623\n",
            "Epoch 24/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4504 - acc: 0.8539 - val_loss: 0.1146 - val_acc: 0.9610\n",
            "Epoch 25/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4380 - acc: 0.8580 - val_loss: 0.0993 - val_acc: 0.9675\n",
            "Epoch 26/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4403 - acc: 0.8590 - val_loss: 0.1111 - val_acc: 0.9638\n",
            "Epoch 27/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4279 - acc: 0.8614 - val_loss: 0.0942 - val_acc: 0.9671\n",
            "Epoch 28/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.4219 - acc: 0.8611 - val_loss: 0.1022 - val_acc: 0.9666\n",
            "Epoch 29/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.4220 - acc: 0.8629 - val_loss: 0.1211 - val_acc: 0.9597\n",
            "Epoch 30/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.4125 - acc: 0.8661 - val_loss: 0.0917 - val_acc: 0.9701\n",
            "Epoch 31/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.4040 - acc: 0.8681 - val_loss: 0.1018 - val_acc: 0.9664\n",
            "Epoch 32/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.4094 - acc: 0.8671 - val_loss: 0.0941 - val_acc: 0.9693\n",
            "Epoch 33/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.4034 - acc: 0.8679 - val_loss: 0.0894 - val_acc: 0.9694\n",
            "Epoch 34/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3964 - acc: 0.8725 - val_loss: 0.0896 - val_acc: 0.9692\n",
            "Epoch 35/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3888 - acc: 0.8733 - val_loss: 0.0912 - val_acc: 0.9707\n",
            "Epoch 36/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3865 - acc: 0.8749 - val_loss: 0.0906 - val_acc: 0.9694\n",
            "Epoch 37/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3850 - acc: 0.8755 - val_loss: 0.0929 - val_acc: 0.9689\n",
            "Epoch 38/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3831 - acc: 0.8756 - val_loss: 0.0931 - val_acc: 0.9668\n",
            "Epoch 39/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3780 - acc: 0.8771 - val_loss: 0.0774 - val_acc: 0.9742\n",
            "Epoch 40/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3707 - acc: 0.8791 - val_loss: 0.0856 - val_acc: 0.9714\n",
            "Epoch 41/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3687 - acc: 0.8815 - val_loss: 0.0809 - val_acc: 0.9728\n",
            "Epoch 42/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3662 - acc: 0.8822 - val_loss: 0.0840 - val_acc: 0.9735\n",
            "Epoch 43/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3620 - acc: 0.8824 - val_loss: 0.0892 - val_acc: 0.9708\n",
            "Epoch 44/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3590 - acc: 0.8828 - val_loss: 0.0830 - val_acc: 0.9711\n",
            "Epoch 45/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3578 - acc: 0.8848 - val_loss: 0.0789 - val_acc: 0.9746\n",
            "Epoch 46/150\n",
            "800/800 [==============================] - 18s 23ms/step - loss: 0.3559 - acc: 0.8843 - val_loss: 0.0757 - val_acc: 0.9742\n",
            "Epoch 47/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3470 - acc: 0.8855 - val_loss: 0.0753 - val_acc: 0.9729\n",
            "Epoch 48/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3474 - acc: 0.8868 - val_loss: 0.0808 - val_acc: 0.9723\n",
            "Epoch 49/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3449 - acc: 0.8886 - val_loss: 0.0764 - val_acc: 0.9744\n",
            "Epoch 50/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3418 - acc: 0.8894 - val_loss: 0.0752 - val_acc: 0.9732\n",
            "Epoch 51/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3360 - acc: 0.8903 - val_loss: 0.0733 - val_acc: 0.9757\n",
            "Epoch 52/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3361 - acc: 0.8914 - val_loss: 0.0710 - val_acc: 0.9770\n",
            "Epoch 53/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3331 - acc: 0.8917 - val_loss: 0.0744 - val_acc: 0.9740\n",
            "Epoch 54/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3314 - acc: 0.8927 - val_loss: 0.0651 - val_acc: 0.9777\n",
            "Epoch 55/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3293 - acc: 0.8932 - val_loss: 0.0655 - val_acc: 0.9773\n",
            "Epoch 56/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3293 - acc: 0.8939 - val_loss: 0.0654 - val_acc: 0.9793\n",
            "Epoch 57/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3228 - acc: 0.8967 - val_loss: 0.0728 - val_acc: 0.9745\n",
            "Epoch 58/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3244 - acc: 0.8949 - val_loss: 0.0756 - val_acc: 0.9745\n",
            "Epoch 59/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3204 - acc: 0.8961 - val_loss: 0.0745 - val_acc: 0.9757\n",
            "Epoch 60/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.3154 - acc: 0.8980 - val_loss: 0.0664 - val_acc: 0.9777\n",
            "Epoch 61/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3195 - acc: 0.8976 - val_loss: 0.0724 - val_acc: 0.9759\n",
            "Epoch 62/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.3102 - acc: 0.8979 - val_loss: 0.0736 - val_acc: 0.9752\n",
            "Epoch 63/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3160 - acc: 0.8973 - val_loss: 0.0658 - val_acc: 0.9771\n",
            "Epoch 64/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3113 - acc: 0.8979 - val_loss: 0.0634 - val_acc: 0.9794\n",
            "Epoch 65/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3077 - acc: 0.9008 - val_loss: 0.0649 - val_acc: 0.9782\n",
            "Epoch 66/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.3075 - acc: 0.9005 - val_loss: 0.0641 - val_acc: 0.9776\n",
            "Epoch 67/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.3018 - acc: 0.9026 - val_loss: 0.0666 - val_acc: 0.9763\n",
            "Epoch 68/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2974 - acc: 0.9041 - val_loss: 0.0626 - val_acc: 0.9797\n",
            "Epoch 69/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.3019 - acc: 0.9020 - val_loss: 0.0693 - val_acc: 0.9765\n",
            "Epoch 70/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2988 - acc: 0.9044 - val_loss: 0.0605 - val_acc: 0.9792\n",
            "Epoch 71/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2977 - acc: 0.9038 - val_loss: 0.0671 - val_acc: 0.9766\n",
            "Epoch 72/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2967 - acc: 0.9038 - val_loss: 0.0617 - val_acc: 0.9785\n",
            "Epoch 73/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2953 - acc: 0.9046 - val_loss: 0.0683 - val_acc: 0.9760\n",
            "Epoch 74/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2896 - acc: 0.9062 - val_loss: 0.0820 - val_acc: 0.9747\n",
            "Epoch 75/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2881 - acc: 0.9071 - val_loss: 0.0615 - val_acc: 0.9794\n",
            "Epoch 76/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2882 - acc: 0.9062 - val_loss: 0.0599 - val_acc: 0.9791\n",
            "Epoch 77/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2883 - acc: 0.9075 - val_loss: 0.0646 - val_acc: 0.9783\n",
            "Epoch 78/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2898 - acc: 0.9060 - val_loss: 0.0613 - val_acc: 0.9797\n",
            "Epoch 79/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2844 - acc: 0.9082 - val_loss: 0.0595 - val_acc: 0.9798\n",
            "Epoch 80/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2900 - acc: 0.9069 - val_loss: 0.0612 - val_acc: 0.9801\n",
            "Epoch 81/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2814 - acc: 0.9084 - val_loss: 0.0610 - val_acc: 0.9795\n",
            "Epoch 82/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2813 - acc: 0.9097 - val_loss: 0.0640 - val_acc: 0.9775\n",
            "Epoch 83/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2806 - acc: 0.9095 - val_loss: 0.0637 - val_acc: 0.9779\n",
            "Epoch 84/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2739 - acc: 0.9095 - val_loss: 0.0612 - val_acc: 0.9795\n",
            "Epoch 85/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2744 - acc: 0.9112 - val_loss: 0.0650 - val_acc: 0.9772\n",
            "Epoch 86/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2761 - acc: 0.9113 - val_loss: 0.0628 - val_acc: 0.9790\n",
            "Epoch 87/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2745 - acc: 0.9107 - val_loss: 0.0563 - val_acc: 0.9807\n",
            "Epoch 88/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2758 - acc: 0.9101 - val_loss: 0.0590 - val_acc: 0.9799\n",
            "Epoch 89/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2737 - acc: 0.9107 - val_loss: 0.0613 - val_acc: 0.9783\n",
            "Epoch 90/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2678 - acc: 0.9136 - val_loss: 0.0583 - val_acc: 0.9797\n",
            "Epoch 91/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2747 - acc: 0.9110 - val_loss: 0.0551 - val_acc: 0.9820\n",
            "Epoch 92/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2674 - acc: 0.9129 - val_loss: 0.0555 - val_acc: 0.9805\n",
            "Epoch 93/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2628 - acc: 0.9151 - val_loss: 0.0594 - val_acc: 0.9805\n",
            "Epoch 94/150\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.2636 - acc: 0.9162 - val_loss: 0.0588 - val_acc: 0.9804\n",
            "Epoch 95/150\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.2664 - acc: 0.9136 - val_loss: 0.0545 - val_acc: 0.9816\n",
            "Epoch 96/150\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.2659 - acc: 0.9149 - val_loss: 0.0535 - val_acc: 0.9803\n",
            "Epoch 97/150\n",
            " 52/800 [>.............................] - ETA: 14s - loss: 0.2641 - acc: 0.9162"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeMvww0fvE8W",
        "colab_type": "code",
        "outputId": "e13bb6cf-5faa-4e5f-f911-8ced250f06c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras.datasets import mnist\n",
        "\n",
        "model = load_model ('drive/My Drive/ggg.hdf5')\n",
        "model.summary()\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_2 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_6 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_7 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_8 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Test loss: 0.02549007553363226\n",
            "Test accuracy: 0.9918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFvqjql7VwYX",
        "colab_type": "code",
        "outputId": "16b66c74-7789-4f61-ebc0-0e6dd3673a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers import Dropout, SpatialDropout2D, Flatten\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from math import exp\n",
        "\n",
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 150\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "# Mandatory to use ImageDataGenerator, it expects 4D Tensors\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "## Data Augmentation with an ImageGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=[1.0,1.2],\n",
        "    horizontal_flip=False)\n",
        "\n",
        "model = load_model ('drive/My Drive/ggg.hdf5')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd=SGD(lr=0.1, decay=0.0, momentum=0.0)\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 50:\n",
        "        return 0.1\n",
        "    elif epoch < 100:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "set_lr = LRS(scheduler)\n",
        "\n",
        "mcp_save1 = ModelCheckpoint ('drive/My Drive/hhh.hdf5', save_best_only = True, monitor = 'val_acc', mode = 'auto')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[set_lr, mcp_save1],\n",
        "                            verbose=1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_2 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_6 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_7 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_8 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n",
            "Epoch 1/150\n",
            "600/600 [==============================] - 20s 34ms/step - loss: 0.1421 - acc: 0.9545 - val_loss: 0.0260 - val_acc: 0.9919\n",
            "Epoch 2/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1430 - acc: 0.9539 - val_loss: 0.0292 - val_acc: 0.9912\n",
            "Epoch 3/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1410 - acc: 0.9537 - val_loss: 0.0291 - val_acc: 0.9903\n",
            "Epoch 4/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1422 - acc: 0.9546 - val_loss: 0.0290 - val_acc: 0.9897\n",
            "Epoch 5/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1413 - acc: 0.9534 - val_loss: 0.0266 - val_acc: 0.9920\n",
            "Epoch 6/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1403 - acc: 0.9535 - val_loss: 0.0290 - val_acc: 0.9912\n",
            "Epoch 7/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1389 - acc: 0.9556 - val_loss: 0.0258 - val_acc: 0.9912\n",
            "Epoch 8/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1391 - acc: 0.9559 - val_loss: 0.0281 - val_acc: 0.9910\n",
            "Epoch 9/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1383 - acc: 0.9543 - val_loss: 0.0258 - val_acc: 0.9912\n",
            "Epoch 10/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1406 - acc: 0.9546 - val_loss: 0.0295 - val_acc: 0.9909\n",
            "Epoch 11/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1365 - acc: 0.9553 - val_loss: 0.0282 - val_acc: 0.9906\n",
            "Epoch 12/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1388 - acc: 0.9541 - val_loss: 0.0306 - val_acc: 0.9902\n",
            "Epoch 13/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1381 - acc: 0.9556 - val_loss: 0.0270 - val_acc: 0.9913\n",
            "Epoch 14/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1367 - acc: 0.9560 - val_loss: 0.0264 - val_acc: 0.9917\n",
            "Epoch 15/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1384 - acc: 0.9549 - val_loss: 0.0272 - val_acc: 0.9916\n",
            "Epoch 16/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1359 - acc: 0.9561 - val_loss: 0.0255 - val_acc: 0.9924\n",
            "Epoch 17/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1370 - acc: 0.9561 - val_loss: 0.0256 - val_acc: 0.9911\n",
            "Epoch 18/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1345 - acc: 0.9563 - val_loss: 0.0259 - val_acc: 0.9913\n",
            "Epoch 19/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1343 - acc: 0.9564 - val_loss: 0.0290 - val_acc: 0.9901\n",
            "Epoch 20/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1360 - acc: 0.9550 - val_loss: 0.0262 - val_acc: 0.9914\n",
            "Epoch 21/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1305 - acc: 0.9572 - val_loss: 0.0271 - val_acc: 0.9914\n",
            "Epoch 22/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1313 - acc: 0.9571 - val_loss: 0.0267 - val_acc: 0.9914\n",
            "Epoch 23/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1294 - acc: 0.9582 - val_loss: 0.0248 - val_acc: 0.9919\n",
            "Epoch 24/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1311 - acc: 0.9576 - val_loss: 0.0292 - val_acc: 0.9910\n",
            "Epoch 25/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1347 - acc: 0.9567 - val_loss: 0.0263 - val_acc: 0.9918\n",
            "Epoch 26/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1347 - acc: 0.9560 - val_loss: 0.0284 - val_acc: 0.9918\n",
            "Epoch 27/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1323 - acc: 0.9574 - val_loss: 0.0246 - val_acc: 0.9919\n",
            "Epoch 28/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1295 - acc: 0.9574 - val_loss: 0.0272 - val_acc: 0.9907\n",
            "Epoch 29/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1309 - acc: 0.9573 - val_loss: 0.0316 - val_acc: 0.9903\n",
            "Epoch 30/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1304 - acc: 0.9582 - val_loss: 0.0259 - val_acc: 0.9920\n",
            "Epoch 31/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1300 - acc: 0.9579 - val_loss: 0.0268 - val_acc: 0.9918\n",
            "Epoch 32/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1277 - acc: 0.9585 - val_loss: 0.0242 - val_acc: 0.9922\n",
            "Epoch 33/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1279 - acc: 0.9588 - val_loss: 0.0263 - val_acc: 0.9916\n",
            "Epoch 34/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1308 - acc: 0.9579 - val_loss: 0.0250 - val_acc: 0.9920\n",
            "Epoch 35/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1284 - acc: 0.9584 - val_loss: 0.0217 - val_acc: 0.9928\n",
            "Epoch 36/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1284 - acc: 0.9590 - val_loss: 0.0230 - val_acc: 0.9921\n",
            "Epoch 37/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1330 - acc: 0.9575 - val_loss: 0.0256 - val_acc: 0.9920\n",
            "Epoch 38/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1291 - acc: 0.9578 - val_loss: 0.0275 - val_acc: 0.9911\n",
            "Epoch 39/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1253 - acc: 0.9596 - val_loss: 0.0262 - val_acc: 0.9914\n",
            "Epoch 40/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1268 - acc: 0.9589 - val_loss: 0.0249 - val_acc: 0.9922\n",
            "Epoch 41/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1265 - acc: 0.9594 - val_loss: 0.0283 - val_acc: 0.9914\n",
            "Epoch 42/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1278 - acc: 0.9580 - val_loss: 0.0226 - val_acc: 0.9924\n",
            "Epoch 43/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1267 - acc: 0.9589 - val_loss: 0.0256 - val_acc: 0.9919\n",
            "Epoch 44/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1302 - acc: 0.9576 - val_loss: 0.0256 - val_acc: 0.9922\n",
            "Epoch 45/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1275 - acc: 0.9593 - val_loss: 0.0259 - val_acc: 0.9914\n",
            "Epoch 46/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1241 - acc: 0.9594 - val_loss: 0.0228 - val_acc: 0.9920\n",
            "Epoch 47/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1266 - acc: 0.9579 - val_loss: 0.0240 - val_acc: 0.9922\n",
            "Epoch 48/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1258 - acc: 0.9590 - val_loss: 0.0240 - val_acc: 0.9917\n",
            "Epoch 49/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1257 - acc: 0.9593 - val_loss: 0.0265 - val_acc: 0.9911\n",
            "Epoch 50/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1284 - acc: 0.9586 - val_loss: 0.0301 - val_acc: 0.9908\n",
            "Epoch 51/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1188 - acc: 0.9612 - val_loss: 0.0250 - val_acc: 0.9918\n",
            "Epoch 52/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1163 - acc: 0.9625 - val_loss: 0.0243 - val_acc: 0.9918\n",
            "Epoch 53/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1157 - acc: 0.9622 - val_loss: 0.0237 - val_acc: 0.9921\n",
            "Epoch 54/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1139 - acc: 0.9630 - val_loss: 0.0235 - val_acc: 0.9920\n",
            "Epoch 55/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1117 - acc: 0.9635 - val_loss: 0.0229 - val_acc: 0.9925\n",
            "Epoch 56/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1128 - acc: 0.9630 - val_loss: 0.0233 - val_acc: 0.9924\n",
            "Epoch 57/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1101 - acc: 0.9641 - val_loss: 0.0230 - val_acc: 0.9920\n",
            "Epoch 58/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1131 - acc: 0.9620 - val_loss: 0.0232 - val_acc: 0.9922\n",
            "Epoch 59/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1103 - acc: 0.9651 - val_loss: 0.0238 - val_acc: 0.9922\n",
            "Epoch 60/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1110 - acc: 0.9639 - val_loss: 0.0240 - val_acc: 0.9917\n",
            "Epoch 61/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1108 - acc: 0.9632 - val_loss: 0.0234 - val_acc: 0.9921\n",
            "Epoch 62/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1139 - acc: 0.9635 - val_loss: 0.0226 - val_acc: 0.9925\n",
            "Epoch 63/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1118 - acc: 0.9638 - val_loss: 0.0227 - val_acc: 0.9924\n",
            "Epoch 64/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1104 - acc: 0.9640 - val_loss: 0.0236 - val_acc: 0.9918\n",
            "Epoch 65/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1098 - acc: 0.9645 - val_loss: 0.0229 - val_acc: 0.9926\n",
            "Epoch 66/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1073 - acc: 0.9644 - val_loss: 0.0232 - val_acc: 0.9922\n",
            "Epoch 67/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1089 - acc: 0.9643 - val_loss: 0.0231 - val_acc: 0.9921\n",
            "Epoch 68/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1085 - acc: 0.9644 - val_loss: 0.0227 - val_acc: 0.9924\n",
            "Epoch 69/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1083 - acc: 0.9650 - val_loss: 0.0222 - val_acc: 0.9925\n",
            "Epoch 70/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1112 - acc: 0.9642 - val_loss: 0.0218 - val_acc: 0.9925\n",
            "Epoch 71/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1057 - acc: 0.9652 - val_loss: 0.0216 - val_acc: 0.9925\n",
            "Epoch 72/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1058 - acc: 0.9651 - val_loss: 0.0219 - val_acc: 0.9925\n",
            "Epoch 73/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1071 - acc: 0.9655 - val_loss: 0.0212 - val_acc: 0.9925\n",
            "Epoch 74/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1069 - acc: 0.9650 - val_loss: 0.0215 - val_acc: 0.9926\n",
            "Epoch 75/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1078 - acc: 0.9661 - val_loss: 0.0215 - val_acc: 0.9927\n",
            "Epoch 76/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1083 - acc: 0.9647 - val_loss: 0.0216 - val_acc: 0.9927\n",
            "Epoch 77/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1111 - acc: 0.9631 - val_loss: 0.0211 - val_acc: 0.9927\n",
            "Epoch 78/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1079 - acc: 0.9652 - val_loss: 0.0206 - val_acc: 0.9928\n",
            "Epoch 79/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1097 - acc: 0.9632 - val_loss: 0.0206 - val_acc: 0.9928\n",
            "Epoch 80/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1088 - acc: 0.9643 - val_loss: 0.0212 - val_acc: 0.9927\n",
            "Epoch 81/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1082 - acc: 0.9646 - val_loss: 0.0219 - val_acc: 0.9923\n",
            "Epoch 82/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1046 - acc: 0.9656 - val_loss: 0.0217 - val_acc: 0.9922\n",
            "Epoch 83/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1087 - acc: 0.9642 - val_loss: 0.0218 - val_acc: 0.9920\n",
            "Epoch 84/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1069 - acc: 0.9649 - val_loss: 0.0214 - val_acc: 0.9923\n",
            "Epoch 85/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1069 - acc: 0.9656 - val_loss: 0.0213 - val_acc: 0.9923\n",
            "Epoch 86/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1072 - acc: 0.9651 - val_loss: 0.0209 - val_acc: 0.9926\n",
            "Epoch 87/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1084 - acc: 0.9654 - val_loss: 0.0209 - val_acc: 0.9923\n",
            "Epoch 88/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1074 - acc: 0.9644 - val_loss: 0.0213 - val_acc: 0.9922\n",
            "Epoch 89/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1045 - acc: 0.9656 - val_loss: 0.0217 - val_acc: 0.9923\n",
            "Epoch 90/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1072 - acc: 0.9653 - val_loss: 0.0206 - val_acc: 0.9923\n",
            "Epoch 91/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1077 - acc: 0.9643 - val_loss: 0.0209 - val_acc: 0.9924\n",
            "Epoch 92/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1065 - acc: 0.9649 - val_loss: 0.0205 - val_acc: 0.9925\n",
            "Epoch 93/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1051 - acc: 0.9655 - val_loss: 0.0214 - val_acc: 0.9923\n",
            "Epoch 94/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1027 - acc: 0.9669 - val_loss: 0.0213 - val_acc: 0.9922\n",
            "Epoch 95/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1059 - acc: 0.9651 - val_loss: 0.0212 - val_acc: 0.9929\n",
            "Epoch 96/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1083 - acc: 0.9648 - val_loss: 0.0208 - val_acc: 0.9924\n",
            "Epoch 97/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1062 - acc: 0.9650 - val_loss: 0.0211 - val_acc: 0.9925\n",
            "Epoch 98/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1047 - acc: 0.9660 - val_loss: 0.0206 - val_acc: 0.9927\n",
            "Epoch 99/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1039 - acc: 0.9660 - val_loss: 0.0206 - val_acc: 0.9928\n",
            "Epoch 100/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1066 - acc: 0.9651 - val_loss: 0.0217 - val_acc: 0.9923\n",
            "Epoch 101/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1032 - acc: 0.9669 - val_loss: 0.0213 - val_acc: 0.9925\n",
            "Epoch 102/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1034 - acc: 0.9659 - val_loss: 0.0217 - val_acc: 0.9924\n",
            "Epoch 103/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1056 - acc: 0.9653 - val_loss: 0.0213 - val_acc: 0.9925\n",
            "Epoch 104/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1105 - acc: 0.9646 - val_loss: 0.0214 - val_acc: 0.9924\n",
            "Epoch 105/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1031 - acc: 0.9664 - val_loss: 0.0215 - val_acc: 0.9924\n",
            "Epoch 106/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1031 - acc: 0.9663 - val_loss: 0.0211 - val_acc: 0.9924\n",
            "Epoch 107/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1052 - acc: 0.9666 - val_loss: 0.0211 - val_acc: 0.9925\n",
            "Epoch 108/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1045 - acc: 0.9664 - val_loss: 0.0211 - val_acc: 0.9923\n",
            "Epoch 109/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1065 - acc: 0.9656 - val_loss: 0.0214 - val_acc: 0.9923\n",
            "Epoch 110/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1033 - acc: 0.9666 - val_loss: 0.0212 - val_acc: 0.9922\n",
            "Epoch 111/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1066 - acc: 0.9654 - val_loss: 0.0212 - val_acc: 0.9925\n",
            "Epoch 112/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1086 - acc: 0.9647 - val_loss: 0.0213 - val_acc: 0.9925\n",
            "Epoch 113/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1068 - acc: 0.9653 - val_loss: 0.0212 - val_acc: 0.9924\n",
            "Epoch 114/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1055 - acc: 0.9662 - val_loss: 0.0208 - val_acc: 0.9926\n",
            "Epoch 115/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1054 - acc: 0.9658 - val_loss: 0.0210 - val_acc: 0.9927\n",
            "Epoch 116/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1045 - acc: 0.9660 - val_loss: 0.0207 - val_acc: 0.9926\n",
            "Epoch 117/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1075 - acc: 0.9656 - val_loss: 0.0210 - val_acc: 0.9925\n",
            "Epoch 118/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1038 - acc: 0.9663 - val_loss: 0.0207 - val_acc: 0.9924\n",
            "Epoch 119/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1059 - acc: 0.9655 - val_loss: 0.0211 - val_acc: 0.9924\n",
            "Epoch 120/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1045 - acc: 0.9661 - val_loss: 0.0209 - val_acc: 0.9927\n",
            "Epoch 121/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1037 - acc: 0.9657 - val_loss: 0.0209 - val_acc: 0.9926\n",
            "Epoch 122/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1064 - acc: 0.9655 - val_loss: 0.0208 - val_acc: 0.9926\n",
            "Epoch 123/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1006 - acc: 0.9668 - val_loss: 0.0211 - val_acc: 0.9927\n",
            "Epoch 124/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1032 - acc: 0.9671 - val_loss: 0.0210 - val_acc: 0.9924\n",
            "Epoch 125/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1042 - acc: 0.9660 - val_loss: 0.0210 - val_acc: 0.9925\n",
            "Epoch 126/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1045 - acc: 0.9666 - val_loss: 0.0208 - val_acc: 0.9926\n",
            "Epoch 127/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1040 - acc: 0.9662 - val_loss: 0.0209 - val_acc: 0.9928\n",
            "Epoch 128/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1077 - acc: 0.9655 - val_loss: 0.0207 - val_acc: 0.9926\n",
            "Epoch 129/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1016 - acc: 0.9665 - val_loss: 0.0210 - val_acc: 0.9927\n",
            "Epoch 130/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1030 - acc: 0.9665 - val_loss: 0.0209 - val_acc: 0.9925\n",
            "Epoch 131/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1028 - acc: 0.9669 - val_loss: 0.0206 - val_acc: 0.9926\n",
            "Epoch 132/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1041 - acc: 0.9656 - val_loss: 0.0208 - val_acc: 0.9926\n",
            "Epoch 133/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1055 - acc: 0.9664 - val_loss: 0.0211 - val_acc: 0.9928\n",
            "Epoch 134/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1065 - acc: 0.9658 - val_loss: 0.0210 - val_acc: 0.9926\n",
            "Epoch 135/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1042 - acc: 0.9656 - val_loss: 0.0205 - val_acc: 0.9927\n",
            "Epoch 136/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1024 - acc: 0.9665 - val_loss: 0.0207 - val_acc: 0.9926\n",
            "Epoch 137/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1049 - acc: 0.9673 - val_loss: 0.0209 - val_acc: 0.9923\n",
            "Epoch 138/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1029 - acc: 0.9668 - val_loss: 0.0209 - val_acc: 0.9926\n",
            "Epoch 139/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1074 - acc: 0.9650 - val_loss: 0.0209 - val_acc: 0.9925\n",
            "Epoch 140/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1036 - acc: 0.9671 - val_loss: 0.0209 - val_acc: 0.9927\n",
            "Epoch 141/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1020 - acc: 0.9665 - val_loss: 0.0207 - val_acc: 0.9924\n",
            "Epoch 142/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1026 - acc: 0.9664 - val_loss: 0.0208 - val_acc: 0.9926\n",
            "Epoch 143/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1053 - acc: 0.9661 - val_loss: 0.0210 - val_acc: 0.9927\n",
            "Epoch 144/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1054 - acc: 0.9655 - val_loss: 0.0209 - val_acc: 0.9925\n",
            "Epoch 145/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1038 - acc: 0.9655 - val_loss: 0.0209 - val_acc: 0.9927\n",
            "Epoch 146/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1057 - acc: 0.9659 - val_loss: 0.0209 - val_acc: 0.9926\n",
            "Epoch 147/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1036 - acc: 0.9661 - val_loss: 0.0208 - val_acc: 0.9927\n",
            "Epoch 148/150\n",
            "600/600 [==============================] - 16s 26ms/step - loss: 0.1030 - acc: 0.9667 - val_loss: 0.0209 - val_acc: 0.9927\n",
            "Epoch 149/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1051 - acc: 0.9656 - val_loss: 0.0207 - val_acc: 0.9925\n",
            "Epoch 150/150\n",
            "600/600 [==============================] - 16s 27ms/step - loss: 0.1027 - acc: 0.9661 - val_loss: 0.0206 - val_acc: 0.9924\n",
            "Test loss: 0.02058185861507427\n",
            "Test accuracy: 0.9924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUIDj_z9ro9_",
        "colab_type": "code",
        "outputId": "30ee93c6-09ab-4173-9257-31c582c01abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras.datasets import mnist\n",
        "\n",
        "model = load_model ('drive/My Drive/hhh.hdf5')\n",
        "model.summary()\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('Train', x_train.shape, y_train.shape)\n",
        "print('Test', (x_test.shape, y_test.shape))\n",
        "\n",
        "print('Train', x_train.min(), x_train.max(), x_train.mean(), x_train.std())\n",
        "print('Test', x_test.min(), x_test.max(), x_test.mean(), x_test.std())\n",
        "\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "\n",
        "print('Train', x_train.shape, y_train.shape)\n",
        "print('Test', (x_test.shape, y_test.shape))\n",
        "\n",
        "print('Train', x_train.min(), x_train.max(), x_train.mean(), x_train.std())\n",
        "print('Test', x_test.min(), x_test.max(), x_test.mean(), x_test.std())\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print('Train', x_train.shape, y_train.shape)\n",
        "print('Test', (x_test.shape, y_test.shape))\n",
        "\n",
        "print('Train', x_train.min(), x_train.max(), x_train.mean(), x_train.std())\n",
        "print('Test', x_test.min(), x_test.max(), x_test.mean(), x_test.std())\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_2 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_6 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_7 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_8 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n",
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Train (60000, 28, 28) (60000,)\n",
            "Test ((10000, 28, 28), (10000,))\n",
            "Train 0 255 33.318421449829934 78.56748998339798\n",
            "Test 0 255 33.791224489795916 79.17246322228644\n",
            "Train (60000, 28, 28, 1) (60000,)\n",
            "Test ((10000, 28, 28, 1), (10000,))\n",
            "Train 0 255 33.318421449829934 78.56748998339798\n",
            "Test 0 255 33.791224489795916 79.17246322228644\n",
            "Train (60000, 28, 28, 1) (60000,)\n",
            "Test ((10000, 28, 28, 1), (10000,))\n",
            "Train 0.0 255.0 33.318447 78.567444\n",
            "Test 0.0 255.0 33.79124 79.172455\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Test loss: 0.021178471290312882\n",
            "Test accuracy: 0.9929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLMXaaarHr6J",
        "colab_type": "code",
        "outputId": "86ce3e14-d2a6-49da-e917-8c18f52f23e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers import Dropout, SpatialDropout2D, Flatten\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from math import exp\n",
        "\n",
        "batch_size = 75\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('Train', x_train.shape, y_train.shape)\n",
        "print('Test', (x_test.shape, y_test.shape))\n",
        "\n",
        "print('Train', x_train.min(), x_train.max(), x_train.mean(), x_train.std())\n",
        "print('Test', x_test.min(), x_test.max(), x_test.mean(), x_test.std())\n",
        "\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=35,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,kernel_size=5,activation='tanh',input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64,kernel_size=5,activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "mcp_save = ModelCheckpoint ('drive/My Drive/convmnist.hdf5', save_best_only = True, monitor = 'val_acc', mode = 'auto')\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[mcp_save],\n",
        "                            verbose=1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (60000, 28, 28) (60000,)\n",
            "Test ((10000, 28, 28), (10000,))\n",
            "Train 0 255 33.318421449829934 78.56748998339798\n",
            "Test 0 255 33.791224489795916 79.17246322228644\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 24, 24, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 64)          51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 184,586\n",
            "Trainable params: 184,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "800/800 [==============================] - 24s 29ms/step - loss: 2.1361 - acc: 0.2218 - val_loss: 1.6170 - val_acc: 0.4862\n",
            "Epoch 2/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 1.7129 - acc: 0.3996 - val_loss: 1.4664 - val_acc: 0.3955\n",
            "Epoch 3/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 1.4911 - acc: 0.4880 - val_loss: 1.2982 - val_acc: 0.4882\n",
            "Epoch 4/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 1.3722 - acc: 0.5353 - val_loss: 1.0911 - val_acc: 0.5794\n",
            "Epoch 5/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 1.2963 - acc: 0.5693 - val_loss: 0.8229 - val_acc: 0.7184\n",
            "Epoch 6/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 1.2497 - acc: 0.5849 - val_loss: 0.8132 - val_acc: 0.7153\n",
            "Epoch 7/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 1.2025 - acc: 0.6025 - val_loss: 0.8143 - val_acc: 0.7124\n",
            "Epoch 8/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 1.1766 - acc: 0.6143 - val_loss: 0.7106 - val_acc: 0.7696\n",
            "Epoch 9/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 1.1499 - acc: 0.6246 - val_loss: 0.6078 - val_acc: 0.8146\n",
            "Epoch 10/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 1.1153 - acc: 0.6347 - val_loss: 0.5210 - val_acc: 0.8565\n",
            "Epoch 11/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 1.0980 - acc: 0.6417 - val_loss: 0.5592 - val_acc: 0.8387\n",
            "Epoch 12/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 1.0715 - acc: 0.6508 - val_loss: 0.4775 - val_acc: 0.8721\n",
            "Epoch 13/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 1.0527 - acc: 0.6576 - val_loss: 0.4488 - val_acc: 0.8759\n",
            "Epoch 14/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 1.0361 - acc: 0.6623 - val_loss: 0.4798 - val_acc: 0.8629\n",
            "Epoch 15/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 1.0180 - acc: 0.6709 - val_loss: 0.4372 - val_acc: 0.8786\n",
            "Epoch 16/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.9971 - acc: 0.6758 - val_loss: 0.4974 - val_acc: 0.8519\n",
            "Epoch 17/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.9854 - acc: 0.6836 - val_loss: 0.3786 - val_acc: 0.9012\n",
            "Epoch 18/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.9655 - acc: 0.6905 - val_loss: 0.3919 - val_acc: 0.8944\n",
            "Epoch 19/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.9397 - acc: 0.6988 - val_loss: 0.4067 - val_acc: 0.8852\n",
            "Epoch 20/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.9254 - acc: 0.7027 - val_loss: 0.3788 - val_acc: 0.8975\n",
            "Epoch 21/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.9124 - acc: 0.7064 - val_loss: 0.3518 - val_acc: 0.9032\n",
            "Epoch 22/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.9010 - acc: 0.7085 - val_loss: 0.3075 - val_acc: 0.9196\n",
            "Epoch 23/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.8927 - acc: 0.7137 - val_loss: 0.3077 - val_acc: 0.9202\n",
            "Epoch 24/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.8636 - acc: 0.7238 - val_loss: 0.2896 - val_acc: 0.9263\n",
            "Epoch 25/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.8613 - acc: 0.7246 - val_loss: 0.3127 - val_acc: 0.9117\n",
            "Epoch 26/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.8322 - acc: 0.7337 - val_loss: 0.2888 - val_acc: 0.9224\n",
            "Epoch 27/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.8328 - acc: 0.7337 - val_loss: 0.2502 - val_acc: 0.9340\n",
            "Epoch 28/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.8126 - acc: 0.7395 - val_loss: 0.2639 - val_acc: 0.9304\n",
            "Epoch 29/50\n",
            "800/800 [==============================] - 17s 21ms/step - loss: 0.8038 - acc: 0.7417 - val_loss: 0.2325 - val_acc: 0.9412\n",
            "Epoch 30/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.7873 - acc: 0.7488 - val_loss: 0.2564 - val_acc: 0.9315\n",
            "Epoch 31/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.7787 - acc: 0.7529 - val_loss: 0.2332 - val_acc: 0.9352\n",
            "Epoch 32/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.7641 - acc: 0.7549 - val_loss: 0.1990 - val_acc: 0.9478\n",
            "Epoch 33/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.7589 - acc: 0.7604 - val_loss: 0.2303 - val_acc: 0.9373\n",
            "Epoch 34/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.7400 - acc: 0.7631 - val_loss: 0.2087 - val_acc: 0.9424\n",
            "Epoch 35/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.7384 - acc: 0.7646 - val_loss: 0.2082 - val_acc: 0.9441\n",
            "Epoch 36/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.7286 - acc: 0.7687 - val_loss: 0.1892 - val_acc: 0.9511\n",
            "Epoch 37/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.7261 - acc: 0.7675 - val_loss: 0.1995 - val_acc: 0.9465\n",
            "Epoch 38/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.7110 - acc: 0.7736 - val_loss: 0.1731 - val_acc: 0.9533\n",
            "Epoch 39/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.7012 - acc: 0.7784 - val_loss: 0.1787 - val_acc: 0.9521\n",
            "Epoch 40/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.6913 - acc: 0.7793 - val_loss: 0.1867 - val_acc: 0.9494\n",
            "Epoch 41/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.6843 - acc: 0.7831 - val_loss: 0.1704 - val_acc: 0.9539\n",
            "Epoch 42/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.6756 - acc: 0.7856 - val_loss: 0.1646 - val_acc: 0.9547\n",
            "Epoch 43/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.6710 - acc: 0.7874 - val_loss: 0.1699 - val_acc: 0.9534\n",
            "Epoch 44/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.6708 - acc: 0.7878 - val_loss: 0.1485 - val_acc: 0.9598\n",
            "Epoch 45/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.6608 - acc: 0.7916 - val_loss: 0.1382 - val_acc: 0.9619\n",
            "Epoch 46/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.6534 - acc: 0.7946 - val_loss: 0.1654 - val_acc: 0.9548\n",
            "Epoch 47/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.6421 - acc: 0.7976 - val_loss: 0.1699 - val_acc: 0.9536\n",
            "Epoch 48/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.6353 - acc: 0.7979 - val_loss: 0.1533 - val_acc: 0.9568\n",
            "Epoch 49/50\n",
            "800/800 [==============================] - 17s 22ms/step - loss: 0.6399 - acc: 0.7973 - val_loss: 0.1360 - val_acc: 0.9614\n",
            "Epoch 50/50\n",
            "800/800 [==============================] - 18s 22ms/step - loss: 0.6302 - acc: 0.8009 - val_loss: 0.1919 - val_acc: 0.9453\n",
            "Test loss: 0.19191021934747696\n",
            "Test accuracy: 0.9453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e41AQq_IgEfo",
        "colab_type": "code",
        "outputId": "8af48487-7b21-4fb3-f2ba-5399af83ca58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers import Dropout, SpatialDropout2D, Flatten\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from math import exp\n",
        "\n",
        "batch_size = 150\n",
        "num_classes = 10\n",
        "epochs = 45\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('Train', x_train.shape, y_train.shape)\n",
        "print('Test', (x_test.shape, y_test.shape))\n",
        "\n",
        "print('Train', x_train.min(), x_train.max(), x_train.mean(), x_train.std())\n",
        "print('Test', x_test.min(), x_test.max(), x_test.mean(), x_test.std())\n",
        "\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=35,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "mcp_save = ModelCheckpoint ('drive/My Drive/convmnistrelu.hdf5', save_best_only = True, monitor = 'val_acc', mode = 'auto')\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[mcp_save],\n",
        "                            verbose=1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (60000, 28, 28) (60000,)\n",
            "Test ((10000, 28, 28), (10000,))\n",
            "Train 0 255 33.318421449829934 78.56748998339798\n",
            "Test 0 255 33.791224489795916 79.17246322228644\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 8, 8, 64)          51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 184,586\n",
            "Trainable params: 184,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/45\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 1.2558 - acc: 0.5695 - val_loss: 0.1536 - val_acc: 0.9599\n",
            "Epoch 2/45\n",
            "400/400 [==============================] - 15s 38ms/step - loss: 0.6033 - acc: 0.8093 - val_loss: 0.0995 - val_acc: 0.9690\n",
            "Epoch 3/45\n",
            "400/400 [==============================] - 16s 39ms/step - loss: 0.4462 - acc: 0.8611 - val_loss: 0.0767 - val_acc: 0.9763\n",
            "Epoch 4/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.3835 - acc: 0.8815 - val_loss: 0.0618 - val_acc: 0.9809\n",
            "Epoch 5/45\n",
            "400/400 [==============================] - 16s 39ms/step - loss: 0.3371 - acc: 0.8956 - val_loss: 0.0572 - val_acc: 0.9808\n",
            "Epoch 6/45\n",
            "400/400 [==============================] - 17s 41ms/step - loss: 0.3000 - acc: 0.9097 - val_loss: 0.0485 - val_acc: 0.9848\n",
            "Epoch 7/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.2810 - acc: 0.9142 - val_loss: 0.0488 - val_acc: 0.9835\n",
            "Epoch 8/45\n",
            "400/400 [==============================] - 16s 39ms/step - loss: 0.2651 - acc: 0.9208 - val_loss: 0.0415 - val_acc: 0.9861\n",
            "Epoch 9/45\n",
            "400/400 [==============================] - 16s 41ms/step - loss: 0.2473 - acc: 0.9238 - val_loss: 0.0392 - val_acc: 0.9873\n",
            "Epoch 10/45\n",
            "400/400 [==============================] - 16s 41ms/step - loss: 0.2321 - acc: 0.9289 - val_loss: 0.0405 - val_acc: 0.9866\n",
            "Epoch 11/45\n",
            "400/400 [==============================] - 17s 41ms/step - loss: 0.2288 - acc: 0.9310 - val_loss: 0.0354 - val_acc: 0.9885\n",
            "Epoch 12/45\n",
            "400/400 [==============================] - 16s 41ms/step - loss: 0.2223 - acc: 0.9331 - val_loss: 0.0365 - val_acc: 0.9875\n",
            "Epoch 13/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.2163 - acc: 0.9352 - val_loss: 0.0407 - val_acc: 0.9853\n",
            "Epoch 14/45\n",
            "400/400 [==============================] - 16s 41ms/step - loss: 0.2080 - acc: 0.9367 - val_loss: 0.0351 - val_acc: 0.9889\n",
            "Epoch 15/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.2027 - acc: 0.9385 - val_loss: 0.0359 - val_acc: 0.9881\n",
            "Epoch 16/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1983 - acc: 0.9401 - val_loss: 0.0318 - val_acc: 0.9904\n",
            "Epoch 17/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1929 - acc: 0.9418 - val_loss: 0.0311 - val_acc: 0.9900\n",
            "Epoch 18/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1920 - acc: 0.9418 - val_loss: 0.0299 - val_acc: 0.9903\n",
            "Epoch 19/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1841 - acc: 0.9436 - val_loss: 0.0314 - val_acc: 0.9900\n",
            "Epoch 20/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1848 - acc: 0.9446 - val_loss: 0.0331 - val_acc: 0.9893\n",
            "Epoch 21/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1785 - acc: 0.9455 - val_loss: 0.0280 - val_acc: 0.9907\n",
            "Epoch 22/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1761 - acc: 0.9466 - val_loss: 0.0322 - val_acc: 0.9897\n",
            "Epoch 23/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1754 - acc: 0.9468 - val_loss: 0.0319 - val_acc: 0.9896\n",
            "Epoch 24/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1743 - acc: 0.9479 - val_loss: 0.0307 - val_acc: 0.9898\n",
            "Epoch 25/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1710 - acc: 0.9479 - val_loss: 0.0345 - val_acc: 0.9884\n",
            "Epoch 26/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1691 - acc: 0.9495 - val_loss: 0.0317 - val_acc: 0.9895\n",
            "Epoch 27/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1690 - acc: 0.9492 - val_loss: 0.0316 - val_acc: 0.9893\n",
            "Epoch 28/45\n",
            "400/400 [==============================] - 16s 41ms/step - loss: 0.1652 - acc: 0.9515 - val_loss: 0.0308 - val_acc: 0.9891\n",
            "Epoch 29/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1632 - acc: 0.9510 - val_loss: 0.0260 - val_acc: 0.9905\n",
            "Epoch 30/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1575 - acc: 0.9530 - val_loss: 0.0290 - val_acc: 0.9896\n",
            "Epoch 31/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1612 - acc: 0.9514 - val_loss: 0.0282 - val_acc: 0.9906\n",
            "Epoch 32/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1594 - acc: 0.9530 - val_loss: 0.0282 - val_acc: 0.9905\n",
            "Epoch 33/45\n",
            "400/400 [==============================] - 16s 41ms/step - loss: 0.1572 - acc: 0.9532 - val_loss: 0.0271 - val_acc: 0.9904\n",
            "Epoch 34/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1538 - acc: 0.9534 - val_loss: 0.0305 - val_acc: 0.9897\n",
            "Epoch 35/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1562 - acc: 0.9538 - val_loss: 0.0259 - val_acc: 0.9901\n",
            "Epoch 36/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1564 - acc: 0.9537 - val_loss: 0.0286 - val_acc: 0.9908\n",
            "Epoch 37/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1495 - acc: 0.9547 - val_loss: 0.0281 - val_acc: 0.9903\n",
            "Epoch 38/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1531 - acc: 0.9546 - val_loss: 0.0270 - val_acc: 0.9908\n",
            "Epoch 39/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1510 - acc: 0.9551 - val_loss: 0.0259 - val_acc: 0.9914\n",
            "Epoch 40/45\n",
            "400/400 [==============================] - 16s 39ms/step - loss: 0.1483 - acc: 0.9549 - val_loss: 0.0281 - val_acc: 0.9899\n",
            "Epoch 41/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1515 - acc: 0.9542 - val_loss: 0.0267 - val_acc: 0.9904\n",
            "Epoch 42/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1472 - acc: 0.9561 - val_loss: 0.0272 - val_acc: 0.9911\n",
            "Epoch 43/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1491 - acc: 0.9565 - val_loss: 0.0286 - val_acc: 0.9897\n",
            "Epoch 44/45\n",
            "400/400 [==============================] - 16s 39ms/step - loss: 0.1474 - acc: 0.9564 - val_loss: 0.0286 - val_acc: 0.9902\n",
            "Epoch 45/45\n",
            "400/400 [==============================] - 16s 40ms/step - loss: 0.1445 - acc: 0.9572 - val_loss: 0.0273 - val_acc: 0.9903\n",
            "Test loss: 0.027317673724133056\n",
            "Test accuracy: 0.9903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMY0u1FPoQI3",
        "colab_type": "code",
        "outputId": "44429508-01fc-40d3-aeec-5413ec29302b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers import Dropout, SpatialDropout2D, Flatten\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from math import exp\n",
        "\n",
        "batch_size = 150\n",
        "num_classes = 10\n",
        "epochs = 45\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('Train', x_train.shape, y_train.shape)\n",
        "print('Test', (x_test.shape, y_test.shape))\n",
        "\n",
        "print('Train', x_train.min(), x_train.max(), x_train.mean(), x_train.std())\n",
        "print('Test', x_test.min(), x_test.max(), x_test.mean(), x_test.std())\n",
        "\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=35,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "model = Sequential()\n",
        "#32\n",
        "model.add(Conv2D(32,(3,3),input_shape=(28,28,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "model.add(Conv2D(32,(3,3))))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "model.add(Conv2D(32,(5,5),strides=2,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "model.add(Dropout(0.4))\n",
        "#64\n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "model.add(Conv2D(64,(5,5),strides=2,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "model.add(Dropout(0.4))\n",
        "#end\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BN())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#sgd=SGD(lr=0.1, decay=0.0, momentum=0.0)\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 80:\n",
        "        return 1e-3\n",
        "    elif epoch < 120:\n",
        "        return 1e-4\n",
        "    elif epoch < 100:\n",
        "        return 1e-5\n",
        "    elif epoch < 100:\n",
        "        return 1e-6    \n",
        "    else:\n",
        "        return 5e-7\n",
        "\n",
        "set_lr = LRS(scheduler)\n",
        "\n",
        "mcp_save1 = ModelCheckpoint ('drive/My Drive/convmnist3.hdf5', save_best_only = True, monitor = 'val_acc', mode = 'auto')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[set_lr, mcp_save1],\n",
        "                            verbose=1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (60000, 28, 28) (60000,)\n",
            "Test ((10000, 28, 28), (10000,))\n",
            "Train 0 255 33.318421449829934 78.56748998339798\n",
            "Test 0 255 33.791224489795916 79.17246322228644\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 12, 12, 32)        25632     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 4, 4, 64)          102464    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 4, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 327,242\n",
            "Trainable params: 326,410\n",
            "Non-trainable params: 832\n",
            "_________________________________________________________________\n",
            "Epoch 1/45\n",
            "400/400 [==============================] - 22s 54ms/step - loss: 1.0754 - acc: 0.6639 - val_loss: 0.1657 - val_acc: 0.9499\n",
            "Epoch 2/45\n",
            "400/400 [==============================] - 19s 47ms/step - loss: 0.3044 - acc: 0.9078 - val_loss: 0.1512 - val_acc: 0.9533\n",
            "Epoch 3/45\n",
            "400/400 [==============================] - 19s 47ms/step - loss: 0.1970 - acc: 0.9405 - val_loss: 0.1136 - val_acc: 0.9650\n",
            "Epoch 4/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.1584 - acc: 0.9523 - val_loss: 0.0479 - val_acc: 0.9856\n",
            "Epoch 5/45\n",
            "400/400 [==============================] - 19s 47ms/step - loss: 0.1363 - acc: 0.9597 - val_loss: 0.0673 - val_acc: 0.9807\n",
            "Epoch 6/45\n",
            "400/400 [==============================] - 19s 47ms/step - loss: 0.1212 - acc: 0.9640 - val_loss: 0.0414 - val_acc: 0.9877\n",
            "Epoch 7/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.1125 - acc: 0.9663 - val_loss: 0.0390 - val_acc: 0.9883\n",
            "Epoch 8/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.1007 - acc: 0.9707 - val_loss: 0.0447 - val_acc: 0.9860\n",
            "Epoch 9/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0970 - acc: 0.9713 - val_loss: 0.0291 - val_acc: 0.9920\n",
            "Epoch 10/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0924 - acc: 0.9729 - val_loss: 0.0489 - val_acc: 0.9846\n",
            "Epoch 11/45\n",
            "400/400 [==============================] - 19s 47ms/step - loss: 0.0846 - acc: 0.9753 - val_loss: 0.0454 - val_acc: 0.9869\n",
            "Epoch 12/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0826 - acc: 0.9760 - val_loss: 0.0344 - val_acc: 0.9899\n",
            "Epoch 13/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0751 - acc: 0.9782 - val_loss: 0.0274 - val_acc: 0.9909\n",
            "Epoch 14/45\n",
            "400/400 [==============================] - 19s 47ms/step - loss: 0.0787 - acc: 0.9776 - val_loss: 0.0357 - val_acc: 0.9896\n",
            "Epoch 15/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0747 - acc: 0.9785 - val_loss: 0.0328 - val_acc: 0.9897\n",
            "Epoch 16/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0712 - acc: 0.9795 - val_loss: 0.0283 - val_acc: 0.9913\n",
            "Epoch 17/45\n",
            "400/400 [==============================] - 19s 47ms/step - loss: 0.0687 - acc: 0.9798 - val_loss: 0.0304 - val_acc: 0.9908\n",
            "Epoch 18/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0677 - acc: 0.9799 - val_loss: 0.0232 - val_acc: 0.9929\n",
            "Epoch 19/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0662 - acc: 0.9804 - val_loss: 0.0227 - val_acc: 0.9928\n",
            "Epoch 20/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0647 - acc: 0.9812 - val_loss: 0.0259 - val_acc: 0.9920\n",
            "Epoch 21/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0639 - acc: 0.9813 - val_loss: 0.0240 - val_acc: 0.9923\n",
            "Epoch 22/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0622 - acc: 0.9826 - val_loss: 0.0309 - val_acc: 0.9903\n",
            "Epoch 23/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0562 - acc: 0.9839 - val_loss: 0.0213 - val_acc: 0.9936\n",
            "Epoch 24/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0591 - acc: 0.9832 - val_loss: 0.0230 - val_acc: 0.9928\n",
            "Epoch 25/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0582 - acc: 0.9828 - val_loss: 0.0226 - val_acc: 0.9929\n",
            "Epoch 26/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0586 - acc: 0.9834 - val_loss: 0.0285 - val_acc: 0.9915\n",
            "Epoch 27/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0571 - acc: 0.9834 - val_loss: 0.0266 - val_acc: 0.9919\n",
            "Epoch 28/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0524 - acc: 0.9848 - val_loss: 0.0235 - val_acc: 0.9929\n",
            "Epoch 29/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0540 - acc: 0.9843 - val_loss: 0.0224 - val_acc: 0.9924\n",
            "Epoch 30/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0527 - acc: 0.9848 - val_loss: 0.0219 - val_acc: 0.9933\n",
            "Epoch 31/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0502 - acc: 0.9853 - val_loss: 0.0225 - val_acc: 0.9930\n",
            "Epoch 32/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0503 - acc: 0.9855 - val_loss: 0.0180 - val_acc: 0.9945\n",
            "Epoch 33/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0483 - acc: 0.9857 - val_loss: 0.0186 - val_acc: 0.9935\n",
            "Epoch 34/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0508 - acc: 0.9857 - val_loss: 0.0218 - val_acc: 0.9935\n",
            "Epoch 35/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0474 - acc: 0.9863 - val_loss: 0.0204 - val_acc: 0.9935\n",
            "Epoch 36/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0500 - acc: 0.9857 - val_loss: 0.0189 - val_acc: 0.9940\n",
            "Epoch 37/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0484 - acc: 0.9862 - val_loss: 0.0230 - val_acc: 0.9934\n",
            "Epoch 38/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0498 - acc: 0.9856 - val_loss: 0.0269 - val_acc: 0.9909\n",
            "Epoch 39/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0437 - acc: 0.9870 - val_loss: 0.0185 - val_acc: 0.9946\n",
            "Epoch 40/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0456 - acc: 0.9872 - val_loss: 0.0196 - val_acc: 0.9940\n",
            "Epoch 41/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0476 - acc: 0.9865 - val_loss: 0.0267 - val_acc: 0.9919\n",
            "Epoch 42/45\n",
            "400/400 [==============================] - 19s 49ms/step - loss: 0.0451 - acc: 0.9864 - val_loss: 0.0185 - val_acc: 0.9943\n",
            "Epoch 43/45\n",
            "400/400 [==============================] - 19s 48ms/step - loss: 0.0436 - acc: 0.9869 - val_loss: 0.0222 - val_acc: 0.9933\n",
            "Epoch 44/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0449 - acc: 0.9869 - val_loss: 0.0195 - val_acc: 0.9940\n",
            "Epoch 45/45\n",
            "400/400 [==============================] - 20s 49ms/step - loss: 0.0441 - acc: 0.9870 - val_loss: 0.0188 - val_acc: 0.9942\n",
            "Test loss: 0.018842263801896478\n",
            "Test accuracy: 0.9942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JWmtFfi_gtk",
        "colab_type": "code",
        "outputId": "75669a8a-497d-488f-83d7-cd90bba1e365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers import Dropout, SpatialDropout2D, Flatten\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from math import exp\n",
        "\n",
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 40\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('Train', x_train.shape, y_train.shape)\n",
        "print('Test', (x_test.shape, y_test.shape))\n",
        "\n",
        "print('Train', x_train.min(), x_train.max(), x_train.mean(), x_train.std())\n",
        "print('Test', x_test.min(), x_test.max(), x_test.mean(), x_test.std())\n",
        "\n",
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=35,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(3,3),input_shape=(28,28,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64,(3,3),input_shape=(28,28,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 80:\n",
        "        return 1e-3\n",
        "    elif epoch < 120:\n",
        "        return 1e-4\n",
        "    elif epoch < 100:\n",
        "        return 1e-5\n",
        "    elif epoch < 100:\n",
        "        return 1e-6    \n",
        "    else:\n",
        "        return 5e-7\n",
        "\n",
        "#set_lr = LRS(scheduler)\n",
        "\n",
        "mcp_save1 = ModelCheckpoint ('drive/My Drive/convmnist5.hdf5', save_best_only = True, monitor = 'val_acc', mode = 'auto')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[mcp_save1],\n",
        "                            verbose=1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (60000, 28, 28) (60000,)\n",
            "Test ((10000, 28, 28), (10000,))\n",
            "Train 0 255 33.318421449829934 78.56748998339798\n",
            "Test 0 255 33.791224489795916 79.17246322228644\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "600/600 [==============================] - 19s 32ms/step - loss: 1.1889 - acc: 0.5976 - val_loss: 0.1667 - val_acc: 0.9576\n",
            "Epoch 2/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.6715 - acc: 0.7860 - val_loss: 0.1249 - val_acc: 0.9601\n",
            "Epoch 3/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.5448 - acc: 0.8304 - val_loss: 0.1165 - val_acc: 0.9634\n",
            "Epoch 4/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.4659 - acc: 0.8555 - val_loss: 0.1033 - val_acc: 0.9684\n",
            "Epoch 5/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.4145 - acc: 0.8724 - val_loss: 0.0789 - val_acc: 0.9738\n",
            "Epoch 6/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.3769 - acc: 0.8847 - val_loss: 0.0764 - val_acc: 0.9746\n",
            "Epoch 7/40\n",
            "600/600 [==============================] - 18s 31ms/step - loss: 0.3528 - acc: 0.8923 - val_loss: 0.0784 - val_acc: 0.9759\n",
            "Epoch 8/40\n",
            "600/600 [==============================] - 19s 32ms/step - loss: 0.3336 - acc: 0.8992 - val_loss: 0.0611 - val_acc: 0.9795\n",
            "Epoch 9/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.3151 - acc: 0.9064 - val_loss: 0.0681 - val_acc: 0.9771\n",
            "Epoch 10/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2997 - acc: 0.9100 - val_loss: 0.0619 - val_acc: 0.9802\n",
            "Epoch 11/40\n",
            "600/600 [==============================] - 18s 31ms/step - loss: 0.2897 - acc: 0.9128 - val_loss: 0.0645 - val_acc: 0.9804\n",
            "Epoch 12/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2812 - acc: 0.9150 - val_loss: 0.0566 - val_acc: 0.9818\n",
            "Epoch 13/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2723 - acc: 0.9180 - val_loss: 0.0619 - val_acc: 0.9813\n",
            "Epoch 14/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2707 - acc: 0.9209 - val_loss: 0.0496 - val_acc: 0.9841\n",
            "Epoch 15/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2603 - acc: 0.9219 - val_loss: 0.0573 - val_acc: 0.9825\n",
            "Epoch 16/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2583 - acc: 0.9242 - val_loss: 0.0526 - val_acc: 0.9831\n",
            "Epoch 17/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2429 - acc: 0.9281 - val_loss: 0.0547 - val_acc: 0.9833\n",
            "Epoch 18/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2418 - acc: 0.9280 - val_loss: 0.0538 - val_acc: 0.9833\n",
            "Epoch 19/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2385 - acc: 0.9283 - val_loss: 0.0508 - val_acc: 0.9850\n",
            "Epoch 20/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2436 - acc: 0.9278 - val_loss: 0.0523 - val_acc: 0.9835\n",
            "Epoch 21/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2359 - acc: 0.9308 - val_loss: 0.0474 - val_acc: 0.9847\n",
            "Epoch 22/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2284 - acc: 0.9322 - val_loss: 0.0501 - val_acc: 0.9839\n",
            "Epoch 23/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2277 - acc: 0.9320 - val_loss: 0.0546 - val_acc: 0.9828\n",
            "Epoch 24/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2216 - acc: 0.9332 - val_loss: 0.0521 - val_acc: 0.9833\n",
            "Epoch 25/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2136 - acc: 0.9357 - val_loss: 0.0472 - val_acc: 0.9853\n",
            "Epoch 26/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2211 - acc: 0.9352 - val_loss: 0.0481 - val_acc: 0.9839\n",
            "Epoch 27/40\n",
            "600/600 [==============================] - 18s 29ms/step - loss: 0.2127 - acc: 0.9375 - val_loss: 0.0518 - val_acc: 0.9831\n",
            "Epoch 28/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2165 - acc: 0.9369 - val_loss: 0.0464 - val_acc: 0.9846\n",
            "Epoch 29/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2102 - acc: 0.9388 - val_loss: 0.0526 - val_acc: 0.9842\n",
            "Epoch 30/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2053 - acc: 0.9398 - val_loss: 0.0554 - val_acc: 0.9818\n",
            "Epoch 31/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2062 - acc: 0.9387 - val_loss: 0.0427 - val_acc: 0.9841\n",
            "Epoch 32/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2013 - acc: 0.9387 - val_loss: 0.0477 - val_acc: 0.9847\n",
            "Epoch 33/40\n",
            "600/600 [==============================] - 18s 29ms/step - loss: 0.2055 - acc: 0.9402 - val_loss: 0.0387 - val_acc: 0.9872\n",
            "Epoch 34/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1988 - acc: 0.9408 - val_loss: 0.0439 - val_acc: 0.9859\n",
            "Epoch 35/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.2022 - acc: 0.9412 - val_loss: 0.0429 - val_acc: 0.9863\n",
            "Epoch 36/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1981 - acc: 0.9421 - val_loss: 0.0436 - val_acc: 0.9853\n",
            "Epoch 37/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1979 - acc: 0.9418 - val_loss: 0.0483 - val_acc: 0.9832\n",
            "Epoch 38/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1940 - acc: 0.9426 - val_loss: 0.0437 - val_acc: 0.9864\n",
            "Epoch 39/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1908 - acc: 0.9443 - val_loss: 0.0527 - val_acc: 0.9821\n",
            "Epoch 40/40\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1949 - acc: 0.9432 - val_loss: 0.0439 - val_acc: 0.9858\n",
            "Test loss: 0.04393346403343021\n",
            "Test accuracy: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FURJ_t9tTshx",
        "colab_type": "code",
        "outputId": "84337654-e050-440a-8811-d714a32d8e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}